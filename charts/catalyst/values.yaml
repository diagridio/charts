---
#-----------------------------------------------------------------------------
# JOIN_TOKEN [REQUIRED]: A join token can be obtained by creating a Region in
# Diagrid Cloud. This token is used by the agent to enroll your installation with
# the Cloud control plane. A join token can be regenerated before you successfully
# complete the installation but not after.
#------------------------------------------------------------------------------
join_token: ""
#------------------------------------------------------------------------------
# Global Options
#------------------------------------------------------------------------------
global:
  # Override the name of the chart
  nameOverride: ""
  # Override the full name of the chart+release
  fullnameOverride: ""
  # Artifacts configuration for container images
  image:
    # registry is the global default image registry for all container images in the chart.
    # When set, this overrides all component-specific registry values.
    # This is useful when using a private registry or mirror.
    # Example: "my-registry.example.com" or "gcr.io/my-project"
    registry: ""
    # pullPolicy is the global default image pull policy for all container images.
    # Can be overridden by individual component image.pullPolicy settings.
    # Valid values: Always, IfNotPresent, Never
    pullPolicy: Always
    # imagePullSecrets is a list of secret names for pulling images from private registries.
    # These secrets must exist in the same namespace as the deployed resources.
    # Reference: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    # Example: [{"name": "my-registry-secret"}]
    imagePullSecrets: []
  # Artifacts configuration for Helm charts
  charts:
    # Registry URL for artifact storage
    # Example: "oci://my-registry.example.com/diagrid/catalyst"
    registry: "oci://public.ecr.aws/diagrid"
    # Basic authentication credentials
    username: ""
    password: ""
    # Certificate-based authentication
    # Inline certificate content (PEM format)
    clientCert: ""
    # Inline private key content (PEM format)
    clientKey: ""
    # Custom CA certificate (PEM format) for self-signed certificates
    customCA: ""
    # Reference to an existing secret containing authentication credentials
    # If provided, inline credentials above will be ignored
    # Expected secret keys:
    #   - username (optional, for basic auth)
    #   - password (optional, for basic auth)
    #   - tls.crt (optional, for cert auth)
    #   - tls.key (optional, for cert auth)
    #   - ca.crt (optional, for custom CA)
    existingSecret: ""
    # Optional: customize the generated secret name
    # secretName: ""
  # Global labels applied to all resources deployed by the chart
  labels: {}
  serviceAccount:
    # Annotations to add to the agent and management service account
    annotations: {}
  # Secrets config for integrating with a secret provider
  secrets:
    # Supported values: kubernetes, aws
    provider: kubernetes
    aws:
      # Required aws region to access AWS Secrets Manager
      region: ""
      # Optional, access key and secret key par to access AWS Secrets Manager
      access_key: ""
      secret_access_key: ""
  # Global control plane URL
  control_plane_namespace: controlplane
  control_plane_url: catalyst-cloud.r1.diagrid.io:443
  control_plane_http_url: https://api.r1.diagrid.io
  # Global sentry configuration
  sentry:
    enabled: true
    endpoint: dapr-sentry.root-dapr-system.svc.cluster.local:443
    remote_endpoint: sentry.r1.diagrid.io:443
    remote_namespace: "controlplane"
    diagrid_trust_anchors_endpoint: https://trust.r1.diagrid.io
    diagrid_trust_anchors_endpoint_insecure: false
    trust_domain: prj-root.trust.diagrid.io
    namespace: root-dapr-system
    trust_anchors_file: "/var/run/secrets/diagrid.io/trust/ca.crt"
  # Global NATS configuration
  nats:
    endpoint: tls://client-events.r1.diagrid.io:443
    trust_domain: trust.diagrid.io
    namespace: "controlplane"
  # Global feature flags
  featureflags:
    nats_enabled: true
    agent_component_validation: false
  # Consolidated image allows you to use a single image for
  # all Catalyst components instead of individual images.
  # This can be used when you need to reduce the number of
  # images you need to procure.
  consolidated_image:
    # enabled determines whether to use a single consolidated image for all components
    enabled: true
    # registry is the default registry for the consolidated image.
    # If global.image.registry is set, it will override this value.
    registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-public
    # repository is the image repository path (without registry or tag).
    # Example: "catalyst-all" or "diagrid/catalyst-all"
    repository: catalyst-all
    # pullPolicy is the image pull policy for the consolidated image
    # If not set, uses global.image.pullPolicy
    pullPolicy: ""
#------------------------------------------------------------------------------
# Shared Configurations
# TODO not implemented yet
#------------------------------------------------------------------------------
shared:
  nodeSelector: {}
  tolerations: []
  affinity: {}
#------------------------------------------------------------------------------
# Agent Configurations
#------------------------------------------------------------------------------
agent:
  logLevel: info
  # This agent config is read directly into the ConfigMap. However,
  # we do apply templating which allows us to provide inline
  # templates in the values.
  config:
    host:
      private_region: true
      ingress_ip_addr: ""
      egress_addresses: []
    placement:
      max_project_count: 50
    project:
      default_managed_pubsub_type: kafka-shared-disabled
      default_managed_state_store_type: postgresql-shared-disabled
      ingress_tls_secret_name: ""
      default_workflow: namespace
    sidecar:
      # default to disable API validation because ingress could be not configured by users
      skip_api_validation: true
      # image_registry is the registry for the sidecar image.
      # If global.image.registry is set, it will be used instead.
      image_registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-public
      # image_name is the image name within the registry.
      # When consolidated_image.enabled is true, uses consolidated_image.repository instead.
      image_name: "{{ ternary .Values.global.consolidated_image.repository \"sidecar\" .Values.global.consolidated_image.enabled }}"
      # image_tag is the sidecar image version
      image_tag: "0.559.0"
      resources:
        limits:
          cpu: 300m
          memory: 256Mi
        requests:
          cpu: 10m
          memory: 25Mi
      service_account_annotations: []
    api_token_error:
      enabled: false
    otel:
      enabled: true
      # image_registry is the registry for the otel collector image.
      # If global.image.registry is set, it will be used instead.
      image_registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-public
      # image_name is the image name within the registry.
      # When consolidated_image.enabled is true, uses consolidated_image.repository instead.
      image_name: "{{ ternary .Values.global.consolidated_image.repository \"diagrid-otel-collector\" .Values.global.consolidated_image.enabled }}"
      # image_tag is the otel collector image version
      image_tag: "0.559.0"
      metrics_endpoint: https://catalyst-metrics.r1.diagrid.io/api/v1/push
      loki_endpoint: https://catalyst-logs.r1.diagrid.io/loki/api/v1/push
      metrics_tls_insecure: false
      loki_tls_insecure: false
      internal_cortex_enabled: false
      internal_cortex_endpoint: ""
      internal_cortex_org_id: common
      loki_enabled: true
      logs_resources:
        limits:
          memory: 750Mi
        requests:
          cpu: 50m
          memory: 500Mi
      metrics_resources:
        limits:
          memory: 1100Mi
        requests:
          cpu: 75m
          memory: 500Mi
    upstream_dapr:
      # container_registry is the registry for upstream Dapr images.
      # Images will be pulled from a dockerhub proxy to avoid rate limits.
      # If global.image.registry is set, it will be used instead.
      # This registry hosts proxy images from daprio on DockerHub
      container_registry: us-central1-docker.pkg.dev/prj-common-d-shared-89549/reg-d-common-docker-hub-proxy/daprio
    internal_dapr:
      # container_registry is the registry for internal Diagrid Dapr images.
      # If global.image.registry is set, it will be used instead.
      container_registry: us-central1-docker.pkg.dev/prj-common-d-shared-89549/reg-d-common-docker-public
      ca:
        trust_anchors_config_map_name: ca-certificates-bundle
      sentry_resources:
        limits:
          memory: 100Mi
      scheduler_resources:
        limits:
          memory: 175Mi
        requests:
          memory: 130Mi
      scheduler:
        # storage_class: ""
        storage_size: 8Gi
    kubernetes:
      qps: 25
      burst: 10
    denylist:
      enabled: false
    featureflags: {}
  metrics:
    port: 9090
  replicaCount: 1
  image:
    # registry is the container registry for the agent image.
    # If global.image.registry is set, it will override this value.
    # Example: "us-central1-docker.pkg.dev/my-project/my-repo"
    registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-public
    # repository is the image name/path within the registry (without registry prefix or tag).
    # Example: "diagrid/cra-agent" or "cra-agent"
    repository: cra-agent
    # tag is the image tag/version to use
    tag: "0.559.0"
    # pullPolicy is the image pull policy for this specific image.
    # If empty, uses global.image.pullPolicy.
    # Valid values: Always, IfNotPresent, Never
    pullPolicy: ""
  nameOverride: ""
  fullnameOverride: ""
  service:
    type: ClusterIP
    port: 9090
  serviceAccount:
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
  podAnnotations:
    prometheus.io/port: "9090"
    prometheus.io/scrape: "true"
  podSecurityContext: {}
  # fsGroup: 2000

  securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

  resources:
    limits:
      memory: 1200Mi
    requests:
      cpu: 40m
      memory: 500Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Golang Soft Memory Limit via env var GOMEMLIMIT
  goSoftLimit:
    percent: 90
  #  override: 600MiB  # it will set GOMEMLIMIT to this value. Supported suffixes, if not using a straight bytes number, are B, KiB, MiB, GiB, and TiB.

  extraEnvs: []
  merge: {}
  patch: {}
#------------------------------------------------------------------------------
# Gateway Configuration
#------------------------------------------------------------------------------
gateway:
  tls:
    enabled: false
    # Name of a previously created secret containing TLS certificates
    existingSecret: ""
    # TLS certificate configuration
    certificates:
      # Name of the TLS certificate file in the secret
      certFile: "tls.crt"
      # Name of the TLS key file in the secret
      keyFile: "tls.key"
      # Optional: Name of the CA certificate file in the secret for client certificate validation
      caFile: ""
    # Optional: customize the generated secret name
    # secretName: ""
    #
    # Inline TLS certificate and key (used if existingSecret is not provided)
    cert: ""
    key: ""
  # Enable High Availability mode for gateway components
  ha:
    enabled: false
    # Minimum number of replicas when HA is enabled
    minReplicas: 2
  identityInjector:
    image:
      # registry is the container registry for the identity injector image.
      # If global.image.registry is set, it will override this value.
      registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-public
      # repository is the image name/path within the registry.
      # When consolidated_image.enabled is true, uses consolidated_image.repository instead.
      repository: identity-injector
      # tag is the image tag/version to use
      tag: "0.559.0"
      # pullPolicy is the image pull policy for this specific image.
      # If empty, uses global.image.pullPolicy.
      pullPolicy: ""
    resources:
      limits:
        cpu: 50m
        memory: 50Mi
      requests:
        cpu: 10m
        memory: 15Mi
  envoy:
    # If HA is enabled, this will be overridden by gateway.ha.minReplicas
    replicaCount: 1
    image:
      # registry is the container registry for the Envoy proxy image.
      # If global.image.registry is set, it will override this value.
      # This uses a DockerHub proxy to avoid rate limits
      registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-hub-proxy
      # repository is the image name/path within the registry.
      repository: envoyproxy/envoy
      # tag is the image tag/version to use
      tag: distroless-v1.33.0
      # pullPolicy is the image pull policy for this specific image.
      # If empty, uses global.image.pullPolicy.
      pullPolicy: ""
    admin:
      enabled: true
      address: "127.0.0.1"
      port: 9090
    serviceAccount:
      name: gateway-envoy
      annotations: {}
    configmap:
      name: envoy-config
    service:
      name: gateway-envoy
      appLabel: envoy
      port: 8080
      targetPort: 8080
      # HTTPS port configuration
      httpsPort: 8443
      httpsTargetPort: 8443
      type: ClusterIP
    xds:
      connect_timeout: 10s
      health_check:
        timeout: 5s
        interval: 15s
        unhealthy_threshold: 2
        healthy_threshold: 1
        path: /healthz
        port: 57205
      connection_keepalive:
        interval: 30s
        timeout: 10s
      hostname: gateway-controlplane
      port: 57206
    podAnnotations: {}
    labels:
      app: gateway-envoy
    selectorLabels:
      app: gateway-envoy
    merge:
      labels: {}
      selectorLabels: {}
    podSecurityContext:
      runAsNonRoot: true
      fsGroup: 2000
    securityContext:
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
    nodeSelector: {}
    affinity: {}
    tolerations: []
    resources:
      limits:
        cpu: 1000m
        memory: 2048Mi
      requests:
        cpu: 100m
        memory: 512Mi
  controlplane:
    # If HA is enabled, this will be overridden by gateway.ha.minReplicas
    replicaCount: 1
    xds:
      mtls:
        enabled: true
    image:
      # registry is the container registry for the gateway control plane image.
      # If global.image.registry is set, it will override this value.
      registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-public
      # repository is the image name/path within the registry.
      # When consolidated_image.enabled is true, uses consolidated_image.repository instead.
      repository: catalyst-gateway
      # tag is the image tag/version to use
      tag: "0.559.0"
      # pullPolicy is the image pull policy for this specific image.
      # If empty, uses global.image.pullPolicy.
      pullPolicy: ""
    serviceAccount:
      name: gateway-controlplane
      annotations: {}
    deployment:
      httpHealthCheck:
        containerPort: 57205
      httpXDS:
        containerPort: 57206
    service:
      name: gateway-controlplane
      appLabel: gateway-controlplane
      httpHealthCheck:
        port: 57205
        targetPort: 57205
      httpXDS:
        port: 57206
        targetPort: 57206
      type: ClusterIP
      clusterIP: None
    podAnnotations: {}
    labels:
      app: gateway-controlplane
    selectorLabels:
      app: gateway-controlplane
    merge:
      labels: {}
      selectorLabels: {}
    podSecurityContext:
      runAsNonRoot: true
      fsGroup: 2000
    securityContext:
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
    nodeSelector: {}
    affinity: {}
    tolerations: []
    resources:
      limits:
        cpu: 100m
        memory: 100Mi
      requests:
        cpu: 50m
        memory: 50Mi
    config: {}
#------------------------------------------------------------------------------
# Management Service Configurations
#------------------------------------------------------------------------------
management:
  logLevel: info
  # This agent mangement is read directly into the ConfigMap. However,
  # we do apply templating which allows us to provide inline
  # templates in the values.
  config:
    http_server:
      enabled: true
      port: 8181
    http_server_auth:
      enabled: true
      domain: "api.r1.diagrid.io"
      audience: "admingrid-api"
    grpc_server:
      enabled: false
    profiling:
      enabled: false
    featureflags: {}
  replicaCount: 2
  image:
    # registry is the container registry for the management image.
    # If global.image.registry is set, it will override this value.
    registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-public
    # repository is the image name/path within the registry (without registry prefix or tag).
    # When consolidated_image.enabled is true, uses consolidated_image.repository instead.
    # Deprecated: This field supports template evaluation for backwards compatibility.
    repository: "{{ ternary .Values.global.consolidated_image.repository \"catalyst-management\" .Values.global.consolidated_image.enabled }}"
    # tag is the image tag/version to use
    tag: "0.559.0"
    # pullPolicy is the image pull policy for this specific image.
    # If empty, uses global.image.pullPolicy.
    pullPolicy: ""
  nameOverride: ""
  fullnameOverride: ""
  service:
    type: ClusterIP
    port: 9090
  http_service:
    type: ClusterIP
    port: 8181
  ingress:
    enabled: false
    dns_top_level_domain: ""
  serviceAccount:
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
  podAnnotations:
    prometheus.io/port: "9091"
    prometheus.io/scrape: "true"
  podSecurityContext: {}
  # fsGroup: 2000

  securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

  resources:
    limits:
      memory: 1200Mi
    requests:
      cpu: 40m
      memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Golang Soft Memory Limit via env var GOMEMLIMIT
  goSoftLimit:
    percent: 90
  #  override: 600MiB  # it will set GOMEMLIMIT to this value. Supported suffixes, if not using a straight bytes number, are B, KiB, MiB, GiB, and TiB.

  extraEnvs: []
  merge: {}
  patch: {}
#------------------------------------------------------------------------------
# Piko Configurations
#------------------------------------------------------------------------------
piko:
  enabled: true
  replicaCount: 1
  pikoName: "piko"
  releaseName: "diagrid-piko"
  # Sets the configmap name and the key in the config map
  # that contains the public key to validate the JWT.
  publicToken:
    # TODO update this
    url: https://tunnels.trust.diagrid.io/.well-known/jwks.json
    cacheTTL: "5m"
  image:
    # registry is the container registry for the Piko image.
    # If global.image.registry is set, it will override this value.
    registry: us-central1-docker.pkg.dev/prj-common-p-shared-79896/reg-p-common-docker-hub-proxy/dotjson
    # repository is the image name/path within the registry.
    repository: piko
    # tag is the image tag/version to use
    tag: "v0.8.2"
    # pullPolicy is the image pull policy for this specific image.
    # If empty, uses global.image.pullPolicy.
    pullPolicy: IfNotPresent
  serviceAccount:
    annotations: {}
    name: "diagrid-piko"
  podAnnotations: {}
  podLabels: {}
  server:
    # -- Specifies the proxy port to listen on. The proxy port listens for
    # requests that are forwarded to upstream services.
    proxyPort: 8000
    # -- Specifies the upstream port to listen on. The upstream port listens
    # for connections from upstream service.
    upstreamPort: 8001
    # -- Specifies the admin port to listen on. The admin port exposes an API for
    # metrics and observability.
    adminPort: 8002
    # -- Specifies the gossip port for inter-node traffic.
    gossipPort: 8003
  certificates:
    # Currently the internal proxy TLS is not supported because the
    # public key comes from the shared control plane and it's not
    # configurable.
    proxy:
      enabled: false
      secretName: ""
  terminationGracePeriodSeconds: 60
  readinessProbe:
    httpGet:
      path: /ready
      port: admin
  affinity: {}
  httpproxy:
    upstreamHostName: "tunnels.api.diagrid.io"
# OpenTelemetry Deployment Collector (Optional) - For Traces and Metrics
#------------------------------------------------------------------------------
# This collector runs as a deployment to handle service-level telemetry
# that is not tied to individual nodes. This avoids the collector doing
# duplicate scraping when used with Prometheus.
opentelemetry-deployment:
  # Set to true to enable traces and metrics collection
  enabled: false
  mode: deployment
  # Scale horizontally for high availability
  replicaCount: 1
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s
    pullPolicy: IfNotPresent
  command:
    name: otelcol-k8s
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  # OpenTelemetry Collector configuration
  # Customize receivers, processors, exporters, and pipelines as needed
  config:
    receivers:
      # OTLP receiver listens for telemetry from applications
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      # Zipkin receiver for compatibility with Zipkin instrumentation
      zipkin:
        endpoint: ${env:MY_POD_IP}:9411
      # Prometheus receiver scrapes metrics from Kubernetes service endpoints
      prometheus:
        config:
          scrape_configs:
            # Scrape pods with prometheus.io annotations
            - job_name: 'kubernetes-pods'
              kubernetes_sd_configs:
                - role: pod
              relabel_configs:
                # Only scrape pods with prometheus.io/scrape=true annotation
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                  action: keep
                  regex: true
                # Use the prometheus.io/path annotation or default to /metrics
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                  action: replace
                  target_label: __metrics_path__
                  regex: (.+)
                # Use the prometheus.io/port annotation or default to the pod port
                - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                  action: replace
                  regex: ([^:]+)(?::\d+)?;(\d+)
                  replacement: $1:$2
                  target_label: __address__
                # Add pod metadata as labels
                - action: labelmap
                  regex: __meta_kubernetes_pod_label_(.+)
                - source_labels: [__meta_kubernetes_namespace]
                  action: replace
                  target_label: kubernetes_namespace
                - source_labels: [__meta_kubernetes_pod_name]
                  action: replace
                  target_label: kubernetes_pod_name
            # Scrape services with prometheus.io annotations
            - job_name: 'kubernetes-services'
              kubernetes_sd_configs:
                - role: service
              relabel_configs:
                # Only scrape services with prometheus.io/scrape=true annotation
                - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                  action: keep
                  regex: true
                # Use the prometheus.io/path annotation or default to /metrics
                - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                  action: replace
                  target_label: __metrics_path__
                  regex: (.+)
                # Use the prometheus.io/port annotation or default to the service port
                - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                  action: replace
                  regex: ([^:]+)(?::\d+)?;(\d+)
                  replacement: $1:$2
                  target_label: __address__
                # Add service metadata as labels
                - action: labelmap
                  regex: __meta_kubernetes_service_label_(.+)
                - source_labels: [__meta_kubernetes_namespace]
                  action: replace
                  target_label: kubernetes_namespace
                - source_labels: [__meta_kubernetes_service_name]
                  action: replace
                  target_label: kubernetes_service_name
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
      memory_limiter:
        check_interval: 1s
        limit_mib: 400
    exporters:
      # Default: Debug exporter for development (writes to stdout)
      # REPLACE THIS with your production backend
      debug:
        verbosity: detailed
        # Example: Send to an OTLP-compatible backend
        # otlphttp:
        #   endpoint: "https://your-backend.example.com"
        #   headers:
        #     api-key: "${env:OTEL_EXPORTER_API_KEY}"
    # Example: Send metrics to Prometheus Remote Write
    # prometheusremotewrite:
    #   endpoint: "https://prometheus.example.com/api/v1/write"
    #   headers:
    #     Authorization: "Bearer ${env:PROMETHEUS_API_KEY}"

    service:
      pipelines:
        # Traces pipeline
        traces:
          receivers: [otlp, zipkin]
          processors: [memory_limiter, batch]
          exporters: [debug] # REPLACE: Change to your traces backend
        # Metrics pipeline
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch]
          exporters: [debug] # REPLACE: Change to your metrics backend
  service:
    type: ClusterIP
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    health-check:
      enabled: true
      containerPort: 13133
      servicePort: 13133
      protocol: TCP
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      protocol: TCP
  presets:
    kubernetesAttributes:
      enabled: true
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8888"
  serviceAccount:
    create: true
#------------------------------------------------------------------------------
# OpenTelemetry DaemonSet Collector (Optional) - For Logs
#------------------------------------------------------------------------------
# This collector runs as a DaemonSet to collect logs from each node's filesystem.
# This ensures comprehensive log collection across the cluster.
opentelemetry-daemonset:
  # Set to true to enable logs collection from all nodes
  enabled: false
  mode: daemonset
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s
    pullPolicy: IfNotPresent
  command:
    name: otelcol-k8s
  # Resources per node (each node runs one collector pod)
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  # OpenTelemetry Collector configuration for logs
  # Customize receivers, processors, exporters, and pipelines as needed
  config:
    receivers:
      # OTLP receiver (optional, for application-sent logs)
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
      memory_limiter:
        check_interval: 1s
        limit_mib: 400
    exporters:
      # Default: Debug exporter for development (writes to stdout)
      # REPLACE THIS with your production logs backend
      debug:
        verbosity: detailed
        # Example: Send logs to Loki
        # loki:
        #   endpoint: "https://loki.example.com/loki/api/v1/push"
        #   headers:
        #     X-Scope-OrgID: "tenant-id"
    # Example: Send to an OTLP-compatible backend
    # otlphttp:
    #   endpoint: "https://your-backend.example.com"
    #   headers:
    #     api-key: "${env:OTEL_EXPORTER_API_KEY}"

    service:
      pipelines:
        # Logs pipeline - collects from local node filesystems
        logs:
          receivers: [otlp, filelog]
          processors: [memory_limiter, k8sattributes, batch]
          exporters: [debug] # REPLACE: Change to your logs backend
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    health-check:
      enabled: true
      containerPort: 13133
      servicePort: 13133
      protocol: TCP
  presets:
    kubernetesAttributes:
      enabled: true
    logsCollection:
      enabled: true
      includeCollectorLogs: false
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8888"
  serviceAccount:
    create: true
#------------------------------------------------------------------------------
# Cleanup Hook Configuration
#------------------------------------------------------------------------------
# Post-delete hook that cleans up resources created by Catalyst
cleanup:
  # enabled determines whether to run the cleanup hook after helm uninstall
  enabled: true
  # ttlSecondsAfterFinished is the TTL for the cleanup job after it finishes
  ttlSecondsAfterFinished: 300
  # namespaces that are created by Catalyst at runtime and should be cleaned up
  namespaces:
    # dapr is the namespace where internal Dapr components are installed
    dapr: "root-dapr-system"
  image:
    # registry is the container registry for the cleanup job image
    # The cleanup job needs both kubectl and helm CLI tools
    registry: docker.io
    # repository is the image repository path
    repository: alpine/k8s
    # tag is the image tag/version
    tag: "1.28.3"
    # pullPolicy is the image pull policy
    pullPolicy: IfNotPresent
